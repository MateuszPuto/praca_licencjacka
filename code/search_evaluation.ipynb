{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msentence_transformers\u001b[39;00m \u001b[39mimport\u001b[39;00m SentenceTransformer\n\u001b[1;32m      2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msentence_transformers\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcross_encoder\u001b[39;00m \u001b[39mimport\u001b[39;00m CrossEncoder\n\u001b[1;32m      3\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mhelper_funcs\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/search/lib/python3.9/site-packages/sentence_transformers/__init__.py:3\u001b[0m\n\u001b[1;32m      1\u001b[0m __version__ \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m2.2.2\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m      2\u001b[0m __MODEL_HUB_ORGANIZATION__ \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39msentence-transformers\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m----> 3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mdatasets\u001b[39;00m \u001b[39mimport\u001b[39;00m SentencesDataset, ParallelSentencesDataset\n\u001b[1;32m      4\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mLoggingHandler\u001b[39;00m \u001b[39mimport\u001b[39;00m LoggingHandler\n\u001b[1;32m      5\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mSentenceTransformer\u001b[39;00m \u001b[39mimport\u001b[39;00m SentenceTransformer\n",
      "File \u001b[0;32m~/.conda/envs/search/lib/python3.9/site-packages/sentence_transformers/datasets/__init__.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mDenoisingAutoEncoderDataset\u001b[39;00m \u001b[39mimport\u001b[39;00m DenoisingAutoEncoderDataset\n\u001b[1;32m      2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mNoDuplicatesDataLoader\u001b[39;00m \u001b[39mimport\u001b[39;00m NoDuplicatesDataLoader\n\u001b[1;32m      3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mParallelSentencesDataset\u001b[39;00m \u001b[39mimport\u001b[39;00m ParallelSentencesDataset\n",
      "File \u001b[0;32m~/.conda/envs/search/lib/python3.9/site-packages/sentence_transformers/datasets/DenoisingAutoEncoderDataset.py:5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mreaders\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mInputExample\u001b[39;00m \u001b[39mimport\u001b[39;00m InputExample\n\u001b[1;32m      4\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnltk\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mnltk\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtokenize\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtreebank\u001b[39;00m \u001b[39mimport\u001b[39;00m TreebankWordDetokenizer\n\u001b[1;32m      8\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39mDenoisingAutoEncoderDataset\u001b[39;00m(Dataset):\n",
      "File \u001b[0;32m~/.conda/envs/search/lib/python3.9/site-packages/nltk/__init__.py:133\u001b[0m\n\u001b[1;32m    125\u001b[0m     subprocess\u001b[39m.\u001b[39mPopen \u001b[39m=\u001b[39m _fake_Popen\n\u001b[1;32m    127\u001b[0m \u001b[39m###########################################################\u001b[39;00m\n\u001b[1;32m    128\u001b[0m \u001b[39m# TOP-LEVEL MODULES\u001b[39;00m\n\u001b[1;32m    129\u001b[0m \u001b[39m###########################################################\u001b[39;00m\n\u001b[1;32m    130\u001b[0m \n\u001b[1;32m    131\u001b[0m \u001b[39m# Import top-level functionality into top-level namespace\u001b[39;00m\n\u001b[0;32m--> 133\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mnltk\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcollocations\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n\u001b[1;32m    134\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mnltk\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdecorators\u001b[39;00m \u001b[39mimport\u001b[39;00m decorator, memoize\n\u001b[1;32m    135\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mnltk\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mfeatstruct\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n",
      "File \u001b[0;32m~/.conda/envs/search/lib/python3.9/site-packages/nltk/collocations.py:36\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mitertools\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39m_itertools\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \u001b[39m# these two unused imports are referenced in collocations.doctest\u001b[39;00m\n\u001b[0;32m---> 36\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mnltk\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmetrics\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[1;32m     37\u001b[0m     BigramAssocMeasures,\n\u001b[1;32m     38\u001b[0m     ContingencyMeasures,\n\u001b[1;32m     39\u001b[0m     QuadgramAssocMeasures,\n\u001b[1;32m     40\u001b[0m     TrigramAssocMeasures,\n\u001b[1;32m     41\u001b[0m )\n\u001b[1;32m     42\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mnltk\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmetrics\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mspearman\u001b[39;00m \u001b[39mimport\u001b[39;00m ranks_from_scores, spearman_correlation\n\u001b[1;32m     43\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mnltk\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mprobability\u001b[39;00m \u001b[39mimport\u001b[39;00m FreqDist\n",
      "File \u001b[0;32m~/.conda/envs/search/lib/python3.9/site-packages/nltk/metrics/__init__.py:18\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mnltk\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmetrics\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39magreement\u001b[39;00m \u001b[39mimport\u001b[39;00m AnnotationTask\n\u001b[1;32m     17\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mnltk\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmetrics\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39maline\u001b[39;00m \u001b[39mimport\u001b[39;00m align\n\u001b[0;32m---> 18\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mnltk\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmetrics\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39massociation\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[1;32m     19\u001b[0m     BigramAssocMeasures,\n\u001b[1;32m     20\u001b[0m     ContingencyMeasures,\n\u001b[1;32m     21\u001b[0m     NgramAssocMeasures,\n\u001b[1;32m     22\u001b[0m     QuadgramAssocMeasures,\n\u001b[1;32m     23\u001b[0m     TrigramAssocMeasures,\n\u001b[1;32m     24\u001b[0m )\n\u001b[1;32m     25\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mnltk\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmetrics\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mconfusionmatrix\u001b[39;00m \u001b[39mimport\u001b[39;00m ConfusionMatrix\n\u001b[1;32m     26\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mnltk\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmetrics\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdistance\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[1;32m     27\u001b[0m     binary_distance,\n\u001b[1;32m     28\u001b[0m     custom_distance,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     35\u001b[0m     presence,\n\u001b[1;32m     36\u001b[0m )\n",
      "File \u001b[0;32m~/.conda/envs/search/lib/python3.9/site-packages/nltk/metrics/association.py:26\u001b[0m\n\u001b[1;32m     23\u001b[0m _SMALL \u001b[39m=\u001b[39m \u001b[39m1e-20\u001b[39m\n\u001b[1;32m     25\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 26\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39mscipy\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mstats\u001b[39;00m \u001b[39mimport\u001b[39;00m fisher_exact\n\u001b[1;32m     27\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mImportError\u001b[39;00m:\n\u001b[1;32m     29\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mfisher_exact\u001b[39m(\u001b[39m*\u001b[39m_args, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m_kwargs):\n",
      "File \u001b[0;32m~/.conda/envs/search/lib/python3.9/site-packages/scipy/stats/__init__.py:485\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[39m.. _statsrefmanual:\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    480\u001b[0m \n\u001b[1;32m    481\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    483\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_warnings_errors\u001b[39;00m \u001b[39mimport\u001b[39;00m (ConstantInputWarning, NearConstantInputWarning,\n\u001b[1;32m    484\u001b[0m                                DegenerateDataWarning, FitError)\n\u001b[0;32m--> 485\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_stats_py\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n\u001b[1;32m    486\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_variation\u001b[39;00m \u001b[39mimport\u001b[39;00m variation\n\u001b[1;32m    487\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mdistributions\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n",
      "File \u001b[0;32m~/.conda/envs/search/lib/python3.9/site-packages/scipy/stats/_stats_py.py:46\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mscipy\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mspecial\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mspecial\u001b[39;00m\n\u001b[1;32m     45\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mscipy\u001b[39;00m \u001b[39mimport\u001b[39;00m linalg\n\u001b[0;32m---> 46\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m distributions\n\u001b[1;32m     47\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m _mstats_basic \u001b[39mas\u001b[39;00m mstats_basic\n\u001b[1;32m     48\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_stats_mstats_common\u001b[39;00m \u001b[39mimport\u001b[39;00m (_find_repeats, linregress, theilslopes,\n\u001b[1;32m     49\u001b[0m                                    siegelslopes)\n",
      "File \u001b[0;32m~/.conda/envs/search/lib/python3.9/site-packages/scipy/stats/distributions.py:10\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m#\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[39m# Author:  Travis Oliphant  2002-2011 with contributions from\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[39m#          SciPy Developers 2004-2011\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[39m#       instead of `git blame -Lxxx,+x`.\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[39m#\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_distn_infrastructure\u001b[39;00m \u001b[39mimport\u001b[39;00m (rv_discrete, rv_continuous, rv_frozen)\n\u001b[0;32m---> 10\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m _continuous_distns\n\u001b[1;32m     11\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m _discrete_distns\n\u001b[1;32m     13\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_continuous_distns\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n",
      "File \u001b[0;32m~/.conda/envs/search/lib/python3.9/site-packages/scipy/stats/_continuous_distns.py:5338\u001b[0m\n\u001b[1;32m   5334\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39m_stats\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m   5335\u001b[0m         \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39minf, np\u001b[39m.\u001b[39minf, np\u001b[39m.\u001b[39mnan, np\u001b[39m.\u001b[39mnan\n\u001b[0;32m-> 5338\u001b[0m levy_l \u001b[39m=\u001b[39m levy_l_gen(b\u001b[39m=\u001b[39;49m\u001b[39m0.0\u001b[39;49m, name\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mlevy_l\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m   5341\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39mlogistic_gen\u001b[39;00m(rv_continuous):\n\u001b[1;32m   5342\u001b[0m     \u001b[39mr\u001b[39m\u001b[39m\"\"\"A logistic (or Sech-squared) continuous random variable.\u001b[39;00m\n\u001b[1;32m   5343\u001b[0m \n\u001b[1;32m   5344\u001b[0m \u001b[39m    %(before_notes)s\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   5363\u001b[0m \n\u001b[1;32m   5364\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/search/lib/python3.9/site-packages/scipy/stats/_distn_infrastructure.py:1954\u001b[0m, in \u001b[0;36mrv_continuous.__init__\u001b[0;34m(self, momtype, a, b, xtol, badvalue, name, longname, shapes, extradoc, seed)\u001b[0m\n\u001b[1;32m   1952\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1953\u001b[0m     dct \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m(distcont)\n\u001b[0;32m-> 1954\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_construct_doc(docdict, dct\u001b[39m.\u001b[39;49mget(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname))\n",
      "File \u001b[0;32m~/.conda/envs/search/lib/python3.9/site-packages/scipy/stats/_distn_infrastructure.py:785\u001b[0m, in \u001b[0;36mrv_generic._construct_doc\u001b[0;34m(self, docdict, shapes_vals)\u001b[0m\n\u001b[1;32m    783\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__doc__\u001b[39m \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__doc__\u001b[39m\u001b[39m.\u001b[39mreplace(\u001b[39m\"\u001b[39m\u001b[39m%(shapes)s\u001b[39;00m\u001b[39m, \u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    784\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 785\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__doc__\u001b[39m \u001b[39m=\u001b[39m doccer\u001b[39m.\u001b[39;49mdocformat(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m\u001b[39m__doc__\u001b[39;49m, tempdict)\n\u001b[1;32m    786\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    787\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mException\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mUnable to construct docstring for \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    788\u001b[0m                     \u001b[39m\"\u001b[39m\u001b[39mdistribution \u001b[39m\u001b[39m\\\"\u001b[39;00m\u001b[39m%s\u001b[39;00m\u001b[39m\\\"\u001b[39;00m\u001b[39m: \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m\n\u001b[1;32m    789\u001b[0m                     (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mname, \u001b[39mrepr\u001b[39m(e))) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/search/lib/python3.9/site-packages/scipy/_lib/doccer.py:65\u001b[0m, in \u001b[0;36mdocformat\u001b[0;34m(docstring, docdict)\u001b[0m\n\u001b[1;32m     63\u001b[0m     newlines \u001b[39m=\u001b[39m [lines[\u001b[39m0\u001b[39m]]\n\u001b[1;32m     64\u001b[0m     \u001b[39mfor\u001b[39;00m line \u001b[39min\u001b[39;00m lines[\u001b[39m1\u001b[39m:]:\n\u001b[0;32m---> 65\u001b[0m         newlines\u001b[39m.\u001b[39mappend(indent\u001b[39m+\u001b[39;49mline)\n\u001b[1;32m     66\u001b[0m     indented[name] \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(newlines)\n\u001b[1;32m     67\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mIndexError\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from sentence_transformers.cross_encoder import CrossEncoder\n",
    "import helper_funcs\n",
    "import torch\n",
    "import numpy as np\n",
    "import faiss\n",
    "import threading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = SentenceTransformer('all-MiniLM-L6-v2', device='cuda')\n",
    "cross_encoder = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')\n",
    "#cross_encoder = CrossEncoder('cross-encoder/stsb-distilroberta-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "d = 384\n",
    "description = \"Flat\"\n",
    "#description = \"IVF1024(PQ16x4),PQ16\"\n",
    "# description = \"HNSW32_SQ8\"\n",
    "# description = \"HNSW32_PQ4\"\n",
    "# description = \"PCA64,IVF32(PQ2x4),Flat\"\n",
    "# description = \"OPQ16,PCA64,IVF32(PQ2x4),Flat\"\n",
    "# description = \"IVF64(PQ4x6),Flat\"\n",
    "# description = \"IVF64(SQ8),Flat\"\n",
    "# description = \"IVF64(PQ6x6),Flat\"\n",
    "\n",
    "index = helper_funcs.create_index(d, description, gpu=True)\n",
    "\n",
    "print(index.ntotal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = []\n",
    "for i in range(1, 10):\n",
    "    f = torch.load(f'../msmarco-vec/msmarco-vectors-{i}.pt')\n",
    "    files.append(f)\n",
    "\n",
    "xb = np.concatenate(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(xb.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def populate_index(xb, index):\n",
    "    faiss.normalize_L2(xb)\n",
    "\n",
    "    index.train(xb)\n",
    "    index.add(xb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "populate_index(xb, index)\n",
    "\n",
    "print(index.ntotal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phase = \"dev.small\"\n",
    "\n",
    "collection = helper_funcs.tsv_to_df(\"../collectionandqueries/collection\", col_name=[\"id\", \"paragraph\"], index_col=\"id\")\n",
    "queries = helper_funcs.tsv_to_df(f\"../collectionandqueries/queries.{phase}\", col_name=[\"query number\", \"query text\"], index_col=\"query number\")\n",
    "qrels = helper_funcs.tsv_to_df(f\"../collectionandqueries/qrels.{phase}\", col_name=[\"query id\", \"del.\", \"document id\", \"del.2\"], index_col=\"query id\").drop(labels=[\"del.\", \"del.2\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_mrr = []\n",
    "\n",
    "def search_thread(query, document):\n",
    "    query_val = query.iloc[0][\"query text\"]\n",
    "    document_val = document.iloc[0][\"paragraph\"]\n",
    "    #reranked_results = helper_funcs.search_and_rerank(query_val, 10, encoder=encoder, cross_encoder=cross_encoder, index=index, collection=collection)\n",
    "    reranked_results = helper_funcs.search(query_val, 10, encoder=encoder, index=index)\n",
    "\n",
    "    result = [reranked_results[i][0][0] for i in range(len(reranked_results))]\n",
    "    result = list(result)\n",
    "    matching_doc = document.iloc[0].name\n",
    "\n",
    "    mrr = helper_funcs.mrr_at(10, result, matching_doc)\n",
    "    total_mrr.append(mrr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating MRR@10\n",
    "generator = helper_funcs.query_document_generator(collection, queries, qrels)\n",
    "\n",
    "for query, document in generator:\n",
    "    t = threading.Thread(target=search_thread, args=[query, document])\n",
    "    t.run()\n",
    "    print()\n",
    "\n",
    "print(f\"MRR@10 {sum(total_mrr) / len(total_mrr)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_mrr"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "search",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16 (main, Mar  8 2023, 14:00:05) \n[GCC 11.2.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "fe094f1b9b9bc75754c2d037302a087284fa1da9906b39b43cf12951366e0342"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
