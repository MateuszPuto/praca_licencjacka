\chapter{Nowoczesne narzędzia wyszukiwania informacji}
\label{chap:drugi}

\section{Komputerowe reprezentacje tekstu}

\subsection{Wektory słów}

Chociaż w niektórych rozwiązania wystarczającą techniką kodowania słów jest potraktowanie ich jako niezależnych wystąpień w słowniku, to istnieją sytuacje, w których możliwość mierzenia odległości syntaktycznej lub semantycznej jest niezaprzeczalnie przydatna. Do tworzenia takich rozproszonych reprezentacji można używać technik sztucznych sieci neuronowych.\newline

Praca naukowa pod przetłumaczonym tytułem 'Efektywna estymacja reprezentacji słów w przestrzeni wektorowej', zdobyła duży rozgłos (ponad 32 tysiące cytowań) dzięki wprowadzeniu interesującego testu na efektywność wektorów słów. We wcześniejszych pracach popularną techniką porównywania jakości nauczonych reprezentacji było mierzenie bliskości zakodowanych słów, które są do siebie najbardziej podobne. Wprowadzona przez autorów technika pozwala na bardziej zniuansowany sposób porównywania takich systemów. Test opiera się na obserwacji mówiącej, iż rozwiązanie zadania określania niektórych typów relacji między słowami można sprowadzić do dodawania i odejmowania wektorów ich reprezentacji. I tak, na przykład, aby zmierzyć rozumienie relacji "Jaki jest wyraz podobny do \emph{mały} w sensie takim jak \emph{największy} jest podobny do \emph{dużego}?", autorzy obliczają wektor \emph{X} uzyskany z reprezentacji w następujący sposób: \emph{X = wektor('duży') - wektor('największy') + wektor('mały')}. Zaproponowane zadanie składa się z prawie 9,000 pytań semantycznych oraz ponad 10,000 pytań syntaktycznych w formie, gdzie na podstawie jednej pary wyrazów trzeba wywnioskować drugi wyraz z drugiej pary. Typem użytej relacji semantycznej są przykładowo: państwo-stolica czy państwo-waluta, natomiast syntaktycznej: przeciwieństwa i zmiany czasu w jakim występuje dany wyraz. Oryginalna metoda proponuje aby uznawać za prawidłowe odpowiedzi tylko te gdzie reprezentacja jest najbliższa do reprezentacji poprawnego wyrazu \autocite{mikolov2013efficient}.\newline

Autorzy zaproponowali również dwa nowe oraz porównali dwa istniejące modele kodowania słów do wektorów. Jednym z wcześniej używanych do tego celu modeli była sieć neuronowa do modelowania języka (NNLM), składająca się z warstw wejścia, projekcji, ukrytej oraz wyjścia. Na wejściu, \emph{N} poprzednich słów jest kodowanych przy pomocy kodowania 1 z n (ang. 'one-hot encoding'), gdzie \emph{n} jest rozmiarem słownika. Następnie warstwa projekcji jest używana w celu przetworzenia danych wejściowych w gęstą reprezentację. O warstwie tej można myśleć jak o wybierającej, w sposób deterministyczny dla danego słowa, związaną z tym słowem kolumnę w macierzy odwzorowania. Kolejno połączone gęste wektory słów są przetwarzane przez warstwę ukrytą sieci, aby zostać zwrócone jako dystrybucja prawdopodobieństwa nad słowami ze słownika. Z powodu różnej częstotliwości występowania słów w języku, przydatnym dla autorów okazało się użycie kodowania Huffmana, gdzie częściej występujące wyrazy otrzymują krótsze kody, tym samym zmniejszając ogólną objętość wstępnie zakodowanego tekstu. W celu przyspieszenia działania tej sieci użyto w tym wypadku również hierarchicznej warstwy \emph{softmax} stworzonej na podstawie drzewa binarnego odpowiadającego kodowaniu Huffmana. Nowe zaproponowane modele to CBOW (ang. 'Continuous Bag-of-Words Model') oraz CSM (ang. 'Continuous Skip-gram Model'). Pierwszy z nich jest podobny do modelu neuronalnego bez warstwy ukrytej. Kody czterech wyrazów z otoczenia lewo, jak i prawostronnego przewidywanego słowa są mapowane przez macierz odwzorowania. Ich reprezentacje zostają połączone przez uśrednienie. Na tych danych działa klasyfikator, próbujący dopasować przesłonięte środkowe słowo. Drugim z modeli jest CSM, który uczy się odwrotnej zależności. Zadanie polega w tym przypadku na wygenerowaniu słów kontekstu. Autorzy użyli, w tym przypadku, klasyfikatora działającego na zmiennej długości lewego i prawego kontekstu wynoszącego od 1 do 10 słów.\autocite{mikolov2013efficient}

%----------------------------------
\subsection{Wektory paragrafów}

Nauczone reprezentacje słów mogą zachowywać relacje między słowami danego języka, a także być przydatne jako dobre początkowe kodowania, na których będzie uczony nowy klasyfikator lub model językowy. Jednak aby w sposób jawny wyłapywać zależności między większymi częściami tekstu, przydatnym byłby sposób osadzania całych zdań, a nawet paragrafów o zmiennej długości. Przed erą nowoczesnych ogromnych modeli językowych (LLM) udanym rozwiązaniem, które pozwalało na uczenie takich zależności był system \emph{Doc2vec} zaprezentowany w pracy 'Dystrybuowane reprezentacje zdań i dokumentów' (ang. 'Distributed Representations of Sentences and Documents').\newline

Wektory paragrafów są motywowane koniecznością reprezentowania danego tekstu jako wektora o określonej długości dla wielu zastosowań takich jak klasyfikacja tekstu. Było to szczególne istotne w latach, gdy nie istniały modele pozwalające na sekwencyjne przetwarzanie dokumentu poprzez obliczenia na pewnej grupie tokenów znajdujących się w kontekście o zadanej wielkości. Mimo istnienia wcześniejszych metod reprezentowania wycinków tekstu były one niezadowalające ze względu na swoje ograniczenia: tracenie ważnych informacji albo ograniczenie do pojedynczych zdań.\autocite{le2014distributed}\newline

Opisywany algorytm działa na podobnej zasadzie jak wcześniejszy model osadzania słów w wektorach. Słowa są mapowane przy użyciu kolumny macierzy \emph{W} do wektora. Dodatkowo \emph{id} paragrafu jest mapowane wykorzystując macierz \emph{D}. Wiąże się to z rozmiarem tej macierzy, która posiada ilość kolumn równą liczbie paragrafów w zbiorze uczącym. Powstały wektor paragrafu służy jako pamięć, która jest w stanie zapisać informacje o brakującym kontekście. Następnie reprezentacje słów, z kontekstu liczącego od kilku do kilkunastu słów, są łączone z wektorem paragrafu poprzez konkatenację lub uśrednianie. Na podstawie tych danych obliczana jest reprezentacja przekrytego słowa. Macierze transformacji są uczone na zbiorze, który powstaje poprzez próbkowanie kontekstu z paragrafu używając stochastycznego algorytmu propagacji wstecznej. Podczas fazy wnioskowania modelu reprezentacja nowego paragrafu musi zostać nauczona, przy innych wagach pozostających bez zmiany. Autorzy używają tej metody w połączeniu z wektorami nauczonymi na zadaniu przewidywania kontekstu z samego wektora paragrafu.\autocite{le2014distributed}\newline

Algorytm uczenia wektorów paragrafów okazał się sukcesem przy użyciu na zbiorach danych dotyczących przewidywania sentymentu: Stanford Sentiment Treebank Dataset, a także zbiorze recenzji IMDb. Metoda osiągnęła statusu najlepszego dostępnego w tamtym czasie rozwiązania na tych zbiorach. W kontekście wyszukiwania informacji ważniejszy jest jednak fakt udanego zastosowania tego modelu do wyszukiwania informacji. Na podstawie niedostępnego publicznie zbioru miliona najpopularniejszych zapytań do wyszukiwarki Google i odpowiadających im dziesięciu najwyżej zwróconych rezultatów, autorzy zaproponowali zadanie podobne w swojej strukturze do późniejszych rozwiązań tego typu. Spośród rezultatów jest wybierana trójka, tak że dwa paragrafy pochodzą z rezultatów jednego zapytania natomiast trzeci z dowolnego paragrafu kolekcji. Celem jest odpowiednie określenie, które z elementów są rezultatami pojedynczego zapytania. Wektory paragrafów są w stanie osiągnąć na tym zbiorze rezultat lepszy od porównywanej metody \emph{bag-of-bigrams} wraz z ważeniem przy pomocy TF-IDF.\autocite{le2014distributed}\newline  


% ------------------------------------------
\section{Modele uczenia maszynowego}

%----------------------------------

\subsection{Transformery}

Było to w pracy naukowej zatytuowanej \emph{Attention is all you need}, gdy w 2017 roku zaprezentowano model sieci neuronowej nazwany \textbf{transformerem}. Składał się on z enkodera i dekodera, złożonego z kilku warstw, gdzie każda posiadała dwie warstwy wewnętrzne: mechanizm uwagi oraz warstwę jednokierunkowej sieci neuronowej. Użyto połączeń rezydualnych, a także normalizacji warstw. Bloki dekodera zostały rozszerzone o dodatkowy mechanizm uwagi, który przy odpowiednim maskowaniu zbiera również informację o kodowaniu z odpowiadających warstw enkodera. Kluczową innowacją był użyty model uwagi pozwalający na łączenie informacji z różnych części zdania, co było utrudnione w rozwiązaniach wykorzystujących inne modele obliczeniowe. Popularne wcześniej sieci rekurencyjne mogą się odnosić oraz propagować gradient tylko względem poprzedzającego tokenu co znacząco utrudnia uczenie długich zależności ze względu na problem zanikających gradientów. Implementacją modelu uwagi, która została użyta jest \emph{uwaga typu zeskalowanego produktu iloczynowego (ang. scaled dot product attention)}. Działa ona zgodnie z poniższym równaniem: 
\autocite{vaswani2017attention}

\begin{equation}
Uwaga(Q, K, V) = softmax(\frac{QK^T}{\sqrt{d_k}}) V
\end{equation}

Macierze \textbf{Q, K, V} składają się kolejno z wielu wektorów zapytania, klucza i wartość. Parametr $\mathbf{d_k}$ odpowiada za skalowanie ze względu na rozmiar macierzy.\newline 

Autorzy używali również wielogłowicowej uwagi (ang. multi-head attention), gdzie każda z głowic operowała na projekcji zapytania, klucza i wartości, które zostały otrzymane przez wymnożenie ich z parametrycznymi macierzami. Wyniki były łączone przez konkatynację i mapowane przez macierz $\mathbf{W^o}$.\newline

Sieć transformera korzysta z nauczonych reprezentacji wektorowych, które odpowiadają wybranej wewnętrznej wielkości reprezentacyjnej modelu $\mathbf{d_{model}}$. Wyjście może natomiast być przetworzone przez warstwę \emph{softmax}, aby zwracało prawdopodobieństwo nad słownikiem. Ponieważ model nie zachowuje naturalnie kolejności tokenów, twórcy używają funkcji sinus i kosinus różnych częstotliwości aby zakodować położenie tokenu.\newline

Już w tej pierwszej pracy na temat nowej architektury autorzy udowodnili jej przydatność osiągając najlepszy w tamtym czasie wynik w translacji pomiędzy francuskim a angielskim, jak i niemieckim a angielskim. Sieć była też bardziej efektywna pod względem zużytych zasobów obliczeniowych do trenowania od istniejących alternatyw. Wytrenowanie jej do poziomu najlepszego istniejącego rozwiązania zajęło autorom zaledwie 3,5 dnia na ośmiu procesorach GPU P100. \autocite{vaswani2017attention}

%-------------------------------------------
\subsection{BERT: Bidirectional Encoder Representations  from Transformers}

BERT, w odróżnieniu od wcześniejszego GPT, jest modelem językowym trenowanym na zarówno lewy jak i również prawym kontekście. Ta właściwość zapewnia potencjalną przewagę nad modelami, które potrafią przewidywać tylko następny token. Zadanie treningowe dla BERT'a jest oparte o test luk (ang. Cloze task). W tej wersji, celem jest wypełnienie losowych luk powstałych poprzez zakrycie tokenów pochodzących z wejściowego ciągu, tak aby zachodziła zgodność między oryginalnymi nieznanymi dla modelu słowami, a tymi które zostały przewidziane przez model.\newline

Styl uczenia się BERT'a oparty jest na dwóch fazach: treningu wstępnego oraz dostrajania. Wzorując się na poprzednich osiągnięciach, które pokazują korzyści płynące z treningu na ogólnym zadaniu z danej dziedziny oraz późniejszym dotrenowaniu architektury do konkretnego zadania końcowego, autorzy proponują rozwiązanie oparte na wspólnym modelu bazowym dającym początek dostrojonym rozwiązaniom różniącym się jedynie dostosowanymi parametrami, a współdzielącymi ogólną architekturę.\newline

Model ten został oparty o sieć transformera. Różni się od oryginalnego, według autorów, praktycznie wyłącznie hiperparametrami. BERT jest przystosowany do rozumienia danych wejściowych w formie krotki: pytanie, odpowiedź. Osiąga to dzięki specjalnemu tokenowi [SEP], który rozdziela zdania oraz zakodowanej reprezentacji, sygnalizującej przynależność do konkretnej części wejścia, która jest dołączona do każdego tokenu. Dodatkowy token klasyfikacji [CLS] jest dołączany do każdej sekwencji wejściowej. Ponieważ transformer potrzebuje jako wejścia sekwencji tokenów, a nie słów, to użyte zostały osadzenia podsłów \emph{WordPiece} \autocite{devlin2018bert}.\newline

Procedura treningowa oparta została na dwóch zadaniach. Pierwsze, odpowiada maskowanemu zadaniu modelowania językowego. Dla 15\% tokenów zakrytych przez specjalny token [MASK] zadaniem jest przewidzenie nieznanych wyrazów. Funkcją nagrody jest entropia krzyżowa. Drugim z wykorzystywanych zadań jest predykcja następującego zdania. Następujące zdanie jest kolejnym zdaniem pochodzącym z tekstu przy prawdopodobieństwie równym 50\%, natomiast w 50\% przypadków zdanie następujące jest losowym zdaniem pochodzącym ze wszystkich dostępnych zdań z korpusu. Problem polega na przewidzeniu czy wybrane dwa zdania następują po sobie. Do treningu wstępnego wykorzystano korpus tekstowy \emph{BooksCorpus}, jak również angielską Wikipedię.\newline

Model oryginalnie dostrojono do aż 11 różnych zadań z zakresu przetwarzania języka naturalnego. Dostrajanie jest wykonywane w procedurze \emph{end-to-end} i jest znacząco mniej kosztowne obliczeniowo od procedury wstępnego treningu. BERT osiągnął również wynik nieosiągalny przedtem na zbiorach danych dotyczących rozumienia tekstu naturalnego: GLUE, SQuAD v1.1, SQuAD v.2.0 i SWAG.\autocite{devlin2018bert}\newline

%-------------------------------------------
\subsection{SBERT}

Model BERT pozwolił na osiągnięcie nowych rekordów w zadaniu semantycznego porównywania tekstu (STS), jednak nie jest przystosowany do wykonywania tego zadania. Głównym problemem uwydatnionym w pracy na temat \emph{Sentence-BERT'a} jest uzyskanie poprawy wydajności obliczeniowej względem BERT'a o kilka rzędów wielkości. Jeśli chcemy badać podobieństwo między częściami tekstu za pomocą podstawowej sieci transformera takiej jak BERT, to musimy podać jej obie części tekstu, a następnie wytrenować klasyfikator na pewnej próbce z tokenów wyjściowych. Problemem jest tu sytuacja gdy, na przykład, chcemy badać podobieństwo między tysiącami zdań. Oznaczałoby to, że musielibyśmy obliczyć wynik podobieństwa dla każdej pary zdań. Kwadratowa złożoność obliczeniowa jest niepożądana i możemy jej uniknąć dzięki modelowi reprezentacyjnemu, takiemu jak SBERT. Drugim podniesionym defektem BERT'a jest fakt, iż nie jest jasnym w jaki sposób należy próbkować jego tokeny wynikowe, żeby otrzymać najlepszą reprezentację. Autorzy próbują kilka klasycznych funkcji: maksimum z tokenów, średnia z tokenów i reprezentacja na specjalnym tokenie [CLS], spośród których metoda uśredniania okazuje się najskuteczniejsza.\autocite{reimers2019sentence}\newline

Sentence-BERT albo krótko SBERT korzysta z syjamskiej (lub potrójnej) architektury sieci neuronowych gdzie blokiem budulcowym każdego z ramion sieci jest wstępnie wytrenowana sieć BERT. Dwa porównywane zdania są przetwarzane przez te sieci, a następnie przy pomocy wybranej strategii próbkowania reprezentacji z niezdekodowanych tokenów wyjściowych otrzymujemy dwie reprezentacje dla każdego ze zdań wejściowych. Wyjście jest dodatkowo rozszerzane o bezwzględną różnicę między reprezentacjami, co poprawia osiągane wyniki. Sieci są dotrenowywane na zadaniu klasyfikacji lub pewnego rodzaju regresji, w zależności od zbioru treningowego. W badaniu SBERT okazał się najlepszą prezentowaną metodą na kilku z zadań STS. Chociaż autorzy zaznaczają, iż SBERT nie jest przeznaczony do tzw. \emph{transfer learningu} to okazuje się, że metoda osiąga najlepsze porównawcze wyniki na większości z wybranych przez nich 7 zadaniach SentEval, dla których sieć nie była trenowana.\autocite{reimers2019sentence}

%-------------------------------------------
\subsection{T5}

Sukcesy ostatnich kilku lat na polu budowania dużych modeli języka, które subiektywnie wydają się być w stanie przejść test Turinga, wynikają w dużej mierze z prostych praw skalowania. Jedną z prób zbadania zależności pomiędzy rodzajem zadania treningowego, zbiorem korpusu tekstów, na których trenuje model, wielkością i budową modelu a osiągniętymi rezultatami na końcowych zadaniach językowych jest T5 czyli 'Text-to-Text Transfer Transformer'. Autorzy tego modelu wykonali ogromną liczbę eksperymentów próbując określić, które z parametrów sprawdzają się najlepiej dla trenowania sieci tego typu. Porównywane wyniki opierają się o pomiar efektów działania na zbiorze zadań takich jak: GLUE i SuperGLUE, SQuAD, abstrakcyjne streszczanie CNN/Daily Mail oraz zbiorach translacji WMT angielsko-niemieckich, ang.-francuskich i ang.-rumuńskich. \autocite{raffel2020exploring} Zgodnie z nazwą zaproponowanego modelu, wszystkie zadania są wprowadzane jako tekst przy użyciu odpowiednich przedrostków, na przykład dla tłumaczenia 'translate English to German: That is good.'. Takie podejście upraszcza użycie jednolitego modelu dla wszystkich zadań i parametrów. Finalnie wybrana architektura, która została wytrenowana na największej liczbie tokenów jest rodzajem Transformera składającego się z enkodera, jak i dekodera. Wytrenowano go wstępnie na zbiorze nazwanym C4, który jest wyczyszczonym zbiorem danych pochodzących z inicjatywy Common Crawl zbierającej ogromne ilości informacji ze stron internetowych, które są upubliczniane każdego miesiąca. Model używa w pewnym stopniu ulepszonego, względem BERT'a, zadania uczenia nienadzorowanego do treningu wstępnego. Polega ono na losowym maskowaniu pewnej części tokenów, które model uczy się przewidywać. Maskowanie jest przeprowadzane tak aby liczba przekrytych słów obok siebie wynosiła średnio trzy. Ulepszono też, cel przewidywań zastępując długie konsekutywne części wejścia specjalnymi tokenami pominięcia, co odciąża model z uzupełniania docelowego tekstu podanymi już wcześniej na wejściu tokenami. T5 jest dostrajany w specjalnym reżimie, w którym widzi najpierw miksturę wszystkich dostrajanych zadań wmieszanych w zbiór C4, a dopiero później jest do nich pojedynczo dostrajany, co nie ma jednak znaczącego wpływu na wynik końcowy względem klasycznego dostrajania. \autocite{raffel2020exploring}

%-------------------------------------------
\section{Zbiory danych}

\subsection{MS MARCO}

Drugim najważniejszym wydarzeniem, oprócz pojawienia się nowego rodzaju sieci do przetwarzania języka, mającym wpływ na zwiększone, w ostatnich kilku latach, zainteresowanie badaniami nad wyszukiwaniem informacji był zbiór danych MS MARCO. Zawiera on 1,010,916 pytań pozyskanych z logów wyszukiwarki Bing. Każde z pytań zawiera odpowiedź napisaną przez człowieka. Co więcej, zbiór ten składa się również 8,841,823 paragrafów pochodzących z 3,563,535 dokumentów, które pozyskano dzięki algorytmom wyszukiwania wyszukiwarki. Dane te są niewątpliwie atrakcyjne dla badaczy opracowujących nowe techniki i modele z dziedziny rozumienia tekstu oraz wyszukiwania informacji. \autocite{bajaj2016ms}\newline

Proces tworzenia tego zbioru polegał na zebraniu dużej puli zapytań wysłanych do wyszukiwarki. Zostały one poddane automatycznemu filtrowaniu, tak aby zawierały jedynie pytania. Następnie z uzyskanych dokumentów internetowych wybrane, przez nowoczesny system, zostały odpowiednie paragrafy. Każde z zapytań zostało przeanalizowane przez człowieka, który na podstawie danych zawartych w dołączonych paragrafach stworzył odpowiedź oraz zaznaczył przydatne paragrafy. Część odpowiedzi została oznaczona do poprawy w procesie recenzji. Takie pytania otrzymały osobną odpowiedź stworzoną przez recenzenta. Do każdego z paragrafów dołączone są: URL, tytuł i tekstowa treść strony, z których pochodzi dany wycinek. Pytania zostały również podzielone na kilka klas przy pomocy klasyfikatora wykorzystującego uczenie maszynowe. \autocite{bajaj2016ms}\newline

MS MARCO (ang. Microsoft MAchine Reading COmprehension) stworzony został aby odpowiadać ogólnie rozumianym potrzebom mierzenia rozumienia tekstu czytanego przez maszyny. Autorzy proponują trzy różne zadania opracowane dla tego zbioru danych. Pierwsze polega na przewidzeniu, przy wykorzystaniu paragrafów, czy można sformułować odpowiedź na ich podstawie oraz na syntezie tejże odpowiedzi. Drugie z zadań ma na celu wygenerowanie poprawnie sformułowanej odpowiedzi, która może być zrozumiana w kontekście pytania i dołączonych paragrafów. Trzecie z zadań, najbliższe obecnym wymaganiom wyszukiwarek internetowych, polega na stworzeniu rankingu paragrafów dla zadanego pytania. \autocite{bajaj2016ms}

% ------------------------------------------
\subsection{SQuAD 2.0}

Nowa wersja znanego zbioru danych SQuAD 1.1 rozszerza swój zakres o pytania, na które nie ma odpowiedzi w podanym kontekście. Taka zmiana pozwoliła znacząco utrudnić zadanie stawianie przed systemem MRC. SQuAD składa się z pytań pozyskanych od zakontraktowanych pracowników społecznych, którzy zadawali pytania do artykułów znajdujących się na stronie Wikipedia, gdzie odpowiedzią jest część tekstu lub pytanie może nie posiadać odpowiedzi. SQuAD 2.0 łączy 100 tysięcy pytań ze zbioru SQuAD 1.1 z 50 tysiącami stworzonych w sposób antagonistyczny pytań, które wyglądają podobnie do reszty pytań, ale na które nie ma odpowiedzi w tekśce. Aby radzić sobie w tym zbiorze, model musi nie tylko umieć odpowiadać na pytania, ale równierz potrafić zdecydować czy posiada wystarczające informacje, żeby udzielić odpowiedzi.

% ------------------------------------------
\section{Przechowywanie gęstych reprezentacji}

\subsection{FAISS}

Faiss jest biblioteką służącą do wyszukiwania podobieństw i analizy skupień operującą na wektorach o gęstych reprezentacjach. \autocite{faiss} Główną zaletą, stworzonego przez zespół Facebook AI (obecnie Meta AI) rozwiązania, są jej możliwości zrównoleglania obliczeń dzięki wykorzystaniu procesora graficznego (GPU). Rozwiązania wykorzystujące jedynie CPU wykonują porównania sekwencyjnie lub z małym poziomem współbieżności co powoduje nieprzystępność takiego podejścia dla wielowymiarowych zbiorów danych takich jak zdjęcia czy wideo, które mogą składać się nawet z miliardów punktów danych. \autocite{johnson2019billion} \newline

Gdy mamy do czynienia z wielowymiarowymi wektorami reprezentacji, porównywanie ich metodą 'brutalnej siły' w celu znalezienia wektorów najbliżej położonych może stawać się niemożliwe. Zakładając, że wykonujemy wcześniej grupowanie wektorów przy pomocy metody k-centroidów moglibyśmy ograniczyć region wyszukiwania do punktów znajdujących się w pobliżu centroidu, od którego jest najbliżej do wektora zapytania. Ta metoda pozwoli na ograniczenie ilości wymaganych porównań. \newline

Technika kwantyzacji zwana 'Product Quantization' (PQ) polega na rozbiciu jednego długiego wektora na wiele podwektorów. W każdej przestrzeni, którą tworzy \emph{i-ta} część wektora wykonujemy algorytm k-centroidów, otrzymując w ten sposób pewną liczbę rozdzielnych podziałów przestrzeni. Reprezentacja każdego z podwektorów jest zastępowana \emph{id} centroidu, do którego należy. W ten sposób po połączeniu numerów id dla \emph{i} części otrzymujemy nowy wektor kodujący, który jest krótszy od oryginalnego. Jest on zwany kodem PQ. Dzięki tym kodom otrzymujemy logarytmiczne zmniejszenie reprezentacji względem długości podstawowych wektorów, co znacząco zmiejszy konieczną wielkość przechowywanych danych. \autocite{productquantaization} \newline

Wyszukiwanie podobnych wektorów, czy to według najmniejszej odległości L2 czy też przy użyciu podobieństwa kosinusowego, składa się z kilku kroków w systemie IVFADC ('Inverted file system with asymmetric distance computation'). Generalna idea polega na przybliżaniu wektora za pomocą podwójnej kwantyzacji.

\begin{equation}
	y \approx q(y) = q_1(y) + q_2(y - q_1(y))
\end{equation}

Pierwszy z nich (\emph{$q_1$}) zwany zgrubnym kwantyzerem, drugi (\emph{$q_2$}) jest kwantyzerem dokładnym przyjmującym jako wejście rezydualne różnice między pierwszą kwantyzacją a danymi wejściowymi.\newline

Grupy kodów PQ dla wszystkich punktów są zapisane w odwróconym indeksie, którego kluczami są centroidy uzyskane poprzez algorytm k-centroidów, gdzie \emph{k} będzie wynosić w tym przypadku około pierwiastek z ilości wszystkich punktów. \autocite{raffel2020exploring} Jednocześnie te przypisania będą służyły za pierwszy, zgrubny poziom kwantyzacji, który będzie określał wartość każdego z punktów jako równą centroidowi, do którego jest przypisany. Drugi poziom kwantyzacji operuje na różnicy między wektorami a ich wartością obliczoną za pomocą pierwszego kwantyzera. Pierwszym powodem dla którego używamy różnic między wektorem a wartością centroidu skupienia, do którego należy jest fakt, iż jest to niedokładność na której operujemy po użyciu pierwszego przybliżenia. Dodatkową przewagą takiego podejścia jest to, że gdy po grupowaniu skupienia leżą w różnych częściach przestrzeni, to po obliczeniu rezydualnej różnicy ich środki zostają nałożone na siebie a pozostałe wartości uzyskanych wektorów są równe oddaleniu od środków dla odpowiednich skupień. \autocite{similaritysearch} Aby znaleźć odległości między wektorami rezydualnymi zapisanymi przy pomocy kodów PQ wykorzystywany jest dystans asymetryczny (ADC). Jest on obliczany jako suma z odległości L2 między wektorem a centroidem kwantyfikacji do którego należy. Ponieważ w kodach PQ każdy z wektorów jest podzielonych na wiele części to odległość będzie się składać z sumy odległości dla każdego z podziałów między wektorem zapytania a zkwantyzowanym wektorem zapisanym w bazie danych. Dopiero po obliczeniu tych wielkości wykonywane jest odpowiednie wyszukiwanie. \autocite{similaritysearch} \autocite{productquantaization} \newline

Autorzy biblioteki Faiss pokazują w kilku eksperymentach na dużych zbiorach danych, że ich przybliżony sposób wyszukiwania najbliższych sąsiadów jest: znacząco szybszy niż odpowiadające rozwiązanie działające na procesorze centralnym (CPU), jest kilkukrotnie efektywniejsze niż wyniki wyszukiwania przeprowadzone innym systemem i w końcu, osiąga czułość większą niż w innym porównywanym badaniu. \autocite{raffel2020exploring} Na podstawie tych wyników można stwierdzić, że biblioteka ta nadaje się do skutecznego wyszukiwania w zbiorach osiągających nawet miliardy wektorów.

% ------------------------------------------
\section{Przyszłość wykorzystania w wyszukiwarkach}

Największy z graczy na rynku usług wyszukiwania rozpoczął wprowadzanie rozwiązań opartych o uczenie maszynowe już kilka lat temu. W 2015 Google rozpoczął wykorzystywanie algorytmu RankBrain. Mimo iż działanie tego systemu rankingowego nie jest oficjalnie znane, to firma przyznała, że służy on do łączenia wyrazów z ich znaczeniami w sposób pozwalający zwracać lepsze wyniki. Inny wprowadzony system nazywany 'neural matching' ma dopasowywać zapytania do stron, biorąc pod uwagę całą zawartość tekstu. W 2019 roku firma wprowadziła BERT'a jako część swojego systemu w dwóch kluczowych miejscach: wyszukiwaniu (ang. 'retrieval') i szeregowaniu (ang. 'ranking'). W dalszej części pracy przyjrzymy się jak BERT może być użyty do osiągania tych celów. Innym z rozwiązań wprowadzonych do usługi na smartfony Google Lens, jest MUM (' Multitask Unified Model'). Jest to model potrafiący rozumieć i generować język w 75 językach, a dodatkowo wykorzystujący inne modalności takie jak rozumienie obrazu. \autocite{howaipowers} Aplikacja pozwala m. in. na rozpoznawanie zdjęć, wyszukiwanie wizualne, tłumaczenie sfotografowanego tekstu.\newline

Microsoft jest wyraźnie zainteresowany podjęciem konkurencji w obszarze wyszukiwania, a jego potencjał wzmacniać mają technologie oparte o AI. Jednym z przykładów tego wysiłku jest stworzenie zbioru MS MARCO. Jednak ostatnimi czasy głośno o wyszukiwarce Bing jest głównie z jednego powodu. Dzięki podjęciu współpracy z OpenAI, firma wypuściła nową wersję swojej wyszukiwarki wzmocnionej o kolejne wcielenie GPT. Sam Microsoft nazywa te ulepszenia kopilotem dla wyszukiwania. I rzeczywiście, oprócz samych linków po wpisaniu zapytania pojawiają się również okienka z wyciągiem najważniejszych odnalezionych informacji. Wprowadzono też opcję czatu znaną z rozwiązań takich jak ChatGPT. Możemy w niej naturalnie, bo przy pomocy języka naturalnego, komunikować się z modelem. Przy jego wykorzystaniu możemy wygenerować różnego rodzaju tekst, jak i zadać pytanie a następnie otrzymać odpowiedź wraz z odsyłaczami do źródeł danych z sieci, które uzasadniają określone fragmenty. \autocite{reinventingsearch} Rozwiązanie to wydaje się oferować ciekawą funkcjonalność, która przynosi powiew świeżości do scementowanego wcześniej świata wyszukiwarek. \newline  

Google nadal posiada wiele przewag konkurencyjnych wraz z byciem, wbrew narracji medialnej, w forpoczcie tworzenia rozwiązań uczenia maszynowego dla wyszukiwania. Jednak bezapelacyjnie szybkość wprowadzenia modeli OpenAI do Bing'u może budzić obawy o to co wydawało się niepodważalnym monopolem. Stąd na początku roku Sundar Pichai ogłosił zwiększone zaangażowanie Alphabet'u w rozwój AI dla wyszukiwania, a szczególnie w ogromne modele językowe. Firma zaprezentowała usługę konkurencyjną do tej oferowanej przez Microsoft. Bard, bo tak nazywa się to rozwiązanie, jest serwisem opartym o model LaMDA. Na razie pozostaje on jednak w fazie testów. \autocite{importantnextstep}

%-------------------------------------------
\subsection{LaMDA}

W następnych rozdziałach podejmiemy się opisania i uruchomienia nowoczesnego systemu wyszukiwania. Należy sobie jednak zadać pytanie czy przyszłość dostępu do informacji opiera się na skalowaniu i ulepszaniu obecnych systemów, czy też będzie wymagać fundamentalnej zmiany podejścia. Jeśli popularność jest jakkolwiek dobrym predyktorem przyszłego sukcesu, to należy się spodziewać, że ChatGPT ma przed sobą świetlaną przyszłość. Szacuje się, że aplikacja osiągnęła 100 milionów użytkowników w styczniu 2023. Zajęło jej to tylko dwa miesiące od premiery, co ustanowiło nowy rekord szybkości adopcji technologii w historii internetu. \autocite{chatgpt100musers} \newline

Ponieważ autorzy ChatuGPT nie udostępnili zweryfikowanej pracy naukowej, która tłumaczyłaby działanie tego rozwiązania, przyjrzymy się pracom prezentującym podobne systemy, których przeznaczeniem jest dialog w języku naturalnym. Jednym z nich jest LaMDA, czyli model językowy przeznaczony do zastosowań dialogowych ('Language Models for Dialog Applications'). Model opisany na początku 2022 roku został oparty o architekturę sieci transformera z samym dekoderem, o wielkości do 137 miliardów parametrów i został wstępnie wytrenowany na 1.56T słów. \autocite{thoppilan2022lamda} LaMDA jest trenowana początkowo jako model językowy ogólnego przeznaczenia na danych składających się z dokumentów tekstowych i dialogów lub ich części. Następnie badacze dotrenowywują model w celu zwiększenia trzech interesujących ich parametrów: jakości, bezpieczeństwa i zaczepienia w faktach. Do tego celu zostają pozyskane interakcje z pracownikami kontraktowymi, którzy przeprowadzają interakcję z systemem oraz oceniają jego odpowiedzi. Następnie, model uczy się przewidywać cechy wygenerowanego tekstu, co umożliwia filtrowanie niepożądanych wyników. Dodatkowo dane dialogowe, które zostały wykorzystane do wczesnego trenowania, zostają ocenione w ten sposób. Następnie model jest dostrajany na tym zbiorze danych. \autocite{thoppilan2022lamda} Aby poprawić zgodność odpowiedzi z rzeczywistością model jest rozszerzony o możliwość używania narzędzi: kalkulatora, tłumacza i narzędzia wyszukiwania informacji. Zwracają one na podstawie zadanego zapytania listę pasujących odpowiedzi. Na bazie zbioru danych zawierającego interakcje z człowiekiem wersja modelu nazwana LaMDA-Reasearch uczy się wyszukiwać odpowiednie informacje. Proces generowania odpowiedzi przebiega więc następująco: podstawowy model LaMDA odpowiada na zadane zapytanie, później LaMDA-Reasearch pyta się o fakty, otrzymuje odpowiedź z narzędzia wyszukiwania, na końcu model generuje ostateczną odpowiedź. \autocite{thoppilan2022lamda} Podsumowując, efekt wykorzystania dodatkowych zadań treningowych ma widoczny wpływ na mierzone metryki takie jak: sensowność, specyficzność, bezpieczeństwo, zakotwiczenie, ciekawość, informatywność. \newline

\subsection{WebGPT}

WebGPT jest systemem otwartego odpowiadania na pytania w długiej formie. Model jest dostrojoną wersją GPT-3, która została wyposażona przy pomocy API o możliwość korzystania z Microsoft Bing. Jest to rozwinięcie pomysłu, o którym wspomnimy w następnym rozdziale, polegającego na wykorzystywaniu modeli językowych do syntezowania odpowiedzi na podstawie informacji, do których mają one dostęp. Środowisko operowania tego systemu, to odpowiadanie na pytania, gdzie model może wykonać kilka różnych akcji: wyszukać daną frazę, wybrać link, znaleźć dany tekst na stronie, zacytować informację, przewijać witryny internetowe oraz udzielać odpowiedzi. Akcje te model może wykonywać za pomocą generowania odpowiednich szablonów tekstowych. Zbiorem danych treningowych są pytania i odpowiedzi z podstrony ELI5 będącej częścią witryny Reddit. Na podstawie tych danych powstało kilka zadań treningowych. Anotacja ludzka została wykorzystana aby porównywać preferencje między odpowiedziami udzielonymi przez człowieka a wygenerowanymi przez model. Na zadania ćwiczebne składa się: klonowanie behawioralne - czyli wykorzystanie zbioru do uczenia nadzorowanego, uczenie modelu nagrody - model jest trenowany, żeby przewidywać która z odpowiedzi jest preferowana przez człowieka, uczenie ze wzmocnieniem przy użyciu algorytmu PPO. W końcu wykorzystywane jest próbkowanie odrzuceniowe. Wybieranych jest w nim \emph{n} próbek spośród których wybierane są te preferowane przez model nagrody. \autocite{nakano2021webgpt} Dzięki temu, że system trenowany jest przy użyciu uczenie ze wzmocnieniem, możliwym staje się uzyskanie preferencji oceniających ponad bazowe odpowiedzi udzielane przez człowieka. Uzyskuje on porównywalne wyniki do ludzkiej demonstracji przebijając ją w 56\% przypadków jeśli chodzi o ogólną użyteczność. Na zbiorze ELI5 69\% pytanych preferuje odpowiedzi WebGPT w porównaniu do najwyżej ocenianego komentarza z danego wątku. \autocite{nakano2021webgpt} 

% ------------------------------------------

% ------------------------------------------